title: Programme
---


Access to lectures and workshop is free, but reservation is mandatory **[here.]({filename}/pages/Venues.md)**
Access to Ircam concerts is through Ircam web page.
Access to Dynamo is through the Dynamo web page.  


- [CONCERTS](#december-2-tuesday)
- [INSTALLATIONS-PERFORMANCES](#december-6-saturday)
- [WORKSHOPS](#december-3-wednesday-workshops)
- [LECTURES](#december-4-thursday-lectures)
- [ABSTRACTS](#abstracts)


## December 2 Tuesday

### 20:00 IMPROTECH CONCERT #1 at Espace de Projection, Ircam

**Psappho, after Iannis Xenakis**  
Lorenzo Colombo percussions  
Marco Fiorini AI-Agents Somax2, live  electronics

**Décembre - premier livre du cycle Arc’**  
Marco Suárez-Cifuentes composition, AI-Agents Somax2  
Johanna Vargas voice  
Nicolas Crosse bass  
Aurélien Gignoux drums  
Nieto texts  

**Traversée III**  
Nicolas Brochec composition, AI-Agents Somax2  
Kanami Koga flute  

**REACHin’ Marseille**  
Turner Williams Jr shahi baaja  
György Kurtág Jr synthetisers  
Jean-Marc Montera guitars, electronics  
The Who/Men (Gerard Assayag, Mikhail Malt, Marco Fiorini AI-Agents Somax2)  
Feat.Mari Kimura, violon, Pierre Couprie, live électronics  

**Deleph**  
Jaap Blonk voice, sound poetry  
Benny Sluchin trombone  
Georges Bloch AI-Agents Omax5, Somax2  

**In albireo luogo…**  
Lara Morciano composition  
Joëlle Léandre, Nicolas Crosse bass  
José-Miguel Fernandez AI-Agents Somax2Collider, immersive electronics  


## December 5 Friday

### 20:00 IMPROTECH CONCERT #2 at Espace de Projection, Ircam


**Artisticiel**  
Bernard Lubat piano, voice  
Gérard Assayag AI-Agents Omax & Somax2  
Marc Chemillier AI-Agents Djazz  

**Solo for sliding Trombone**  
After John Cage composition  
Benny Sluchin trombone  
Mikhail Malt AI-Agents Somax2,  immersive electronics  

**Taideji**  
Lara Morciano compposition, piano  
Thierry Miroglio percussions  
Jose-Miguel Fernandez AI-Agents Somax2, immersive electronics  

**Transe III**  
Justin Vali Malagasy Zither, voice  
Marc Chemillier AI-Agents Djazz  
NSDOS electronic hack, live coding, dance  

**NaN - Not a Number**  
Alberto Gatti composition, immersive electronics, AI-Agents Somax2  
Anaïs del Sordo voice  

**in memoriam Susan Alcorn**  
Miya Masaoka composition, koto  

**SoVo II**
George Lewis composition  
Joëlle Léandre bass, voice
Miya Masaoka koto  
Marco Fiorini, SoVo AI system, pianos Disklavier, immersive audio

## December 7 Sunday

### 18:00 IMPROTECH CONCERT #3 at La Dynamo, Pantin

**REACHin' Paris**  
Steve Lehman, Saxophones  
Miles Okazaki, Guitar, Somax2 AI Agents  
The Somax Brothers (Gérard Assayag, Marco FIorini) Somax2 AI Agents  

*This concert follows a Master-Class in the afternoon with the same musicians.*

## December 6 Saturday

### 14:00  - 19:00 INSTALLATIONS / PERFORMANCES in Studio 5, Ircam  

**WWW**  
José-Miguel Fernandez composition, AI-Agents Somax2Collider, immersive electronics  
Amaryllis Billet, Philippe Bord, Nicolas Chedmail, Roméo Monteiro, Maxime Morel, Joris Rühl, Spat'Sonore physical spatialisation installation  

**Batterie Fragile**  
Yves Chaudouet inventor of the Fragile Drum  
Jean-Brice Godet, clarinettes  
Thierry Miroglio, Aurelien Gignoux, percussions  
NOSFELL, voice
Somax Brothers, AI Agents Somax2 / Somax2Collider  


## December 3 Wednesday Workshops

### 10h - 13h Workshop *Resounding Bodies*  in Studio 2, Ircam  

Alberto Gatti, composer, designer  
Anaïs del Sordo, voice, body harness  
Marco Fiorini Spatial Agents Somax2  

### 14h - 19h Workshops in Studio 5, Ircam 

**Breathing Media Project**  
Tamami Tono, Sho  
Minako Ito, Bugaku dance  
Todokoro, ideo  

**Mugic Magic**  
Mari Kimura, Mugic sensors, violon  


**Sophtar**  
Federico Visi, Sophtar Instrument, Somax2  

**First Meeting**  
Alain Blesing, Guitare électrique, Somax2  
Claudie Boucau, flutes

## December 4 Thursday Workshops
### 14h - 19h Workshops in Studio 5, Ircam 

**Boulez Rebooted and other projects**  
Levy Lorenzo, percussion, live electronics, somax2

**PURE MALT: Augmented Improvisations, from instrumental gesture to telematics** 
Mikhail Malt, Somax2, live electronics, telematics  
Cassia Carrascoza, flute, telematics  
Li Chin Li, sheng  

** DjazzTok  **  
Yohann Rabearivelo, AI Agents Djazz, TikTok, Video  
Heny Zouary, Violin  


**Activate Cities**  
NSDOS (Kirikou Des) electro-hack, dance
Noam Assayag litterature, voice  


## December 5 Friday Workshops
### 11h - 13h Workshops in Studio 5, Ircam 

**Metagorminx**  
Gyorgy Kurtag, synthesizers  
Donatien Garnier, Metagorminx instrument, poetry  
Emmanuelle Pépin, dance  

**Sardines**  
Guillaume Piccarreta, synthesizers  
Raphaël Forment, Rémi Georges, live coding


## December 4 Thursday Lectures
### 9h30 - 13h Lectures in Stravinsky hall, Ircam 

**Méthodes Co-Créatives pour des Partenariats Musicaux Humain-IA de Longue Durée  (ANR MICCDroP)**  
Ken Deguernel, Université de Lille  

**Generative Spatial Synthesis of Sound and Music (ERC G3S)**  
Alain Bonardi, Université Paris 8  

**Xenakis Reloaded in the age of AI, the case of Psappho**  
Makis Solomos, Université Paris 8  

**Inside Artificial Improvisation (ANR InAI)**  
Pierre Saint-Germier, CNRS, Ircam  

**Gestures in Electronic Improvised Music**  
Pierre Couprie, Evry Paris-Saclay University

**Extended Instrumental Techniques Recognition for Generative Coimprovisation**  
Nicolas Brochec, Geidai University Tokyo  
Marco Fiorini, Ircam, Collegium Musicae  


## December 5 Friday lectures
### 14h - 19h Lectures in Stravinsky hall, Ircam

**Like Flesh, Nine Jewel Deers, composition and improvisation**  
Sivan Eldar, composer  
Jean-Louis Giavitto, CNRS Ircam  
Augustin Muller, Ircam   

**The Odd Couple - Human & AI Making Music in the Moment**  
Oded Ben Tal, Kingston University, London  
David Dolan, pianist, Guildhall School of Music & Drama London  

**Unity interface to Generative Improvisation Agents**  
Daniel Brown, University de Picardie  

**ArchiMusic Reloaded with no-code generative AI**  
Jean-Rémy Guédon, composer, musician  

**Evaluation of AI based improvisation systems**  
Gilbert Nouno, Haute Ecole de Musique de Genève (HEM)
Christophe Fellay, École de design et Haute école d'art du Valais (EDHEA) 

Extense : collective and human-machine agentivity  
Nicolas Souchal, Diemo Schwartz, Ircam


## Abstracts

### Concerts

**Décembre - premier livre du cycle Arc’** est un projet qui intègre musique écrite et improvisation, inspiré par l’univers graphique de Daichi Mori, en particulier par l’un de ses dessins : un emakimono au titre énigmatique, La Naissance du Centaure. Décembre en constitue le premier chapitre ouvrant le cycle. Conçu pour et avec trois musiciens  interagissant avec un dispositif électroacoustique, qui spatialise une multiplicité de voix improvisées par l’IA, cette œuvre est une étape d’une recherche portée sur la voix et la parole dans les systèmes d’improvisation génératifs, ainsi que sur la mise en espace des structures musicales générées dans l'interaction avec le logiciel Somax2..
 
**Traversée III** s’inscrit dans une série de pièces pour flûte et dispositif électronique en temps réel, dans lesquelles la génération à la volée du matériau électronique repose sur la reconnaissance automatique par IA des modes de jeu de la flûte. Alors que, dans les pièces précédentes, cette génération était directement pilotée par le système de reconnaissance, Traversée III s’en distingue par l’intégration d’agents improvisateurs artificiels Somax2 qui assument désormais ce rôle. Ces agents interagissent avec la partie instrumentale avec des réponses allant du contrepoint à l’accompagnement, en fonction des modes de jeu interprétés. Il en résulte parfois l’introduction d’événements inattendus, susceptibles d’influencer, ou non, la partie de flûte et d’en modifier le déroulement.
 
**Artisticiel** This set-up is at the heart of the CD-book Artisticiel, a manifesto for cocreative interaction, with Bernard Lubat and Ircam / EHESS researchers Gérard Assayag and Marc Chemillier operating computer systems (OMax, SoMax, Djazz) capable of « listening » carefully, capturing the game of improvising musicians in real time, in order to produce new improvisations combining imitations and transformations. In direct contact with the human performers, the computer learns how to play, making steady progress toward musical expertise, tirelessly increasing the amount of structured information stored in its database, while human operators are still in control of global strategies (when to choose silence, responsory or heterophony, scarcity or complexity etc.)
 
**Transe III** World Music star zitherist Justin Vali will propel this collective into the realms of musical trance as practiced in Madagascar. The crystalline swirls of Justin Vali's zither are relayed by the obsessive improvisations generated by Marc Chemillier using the Djazz artificial intelligence system, trained through machine learning with zither playing data. This captivating dialog is supported by the hypnotic grooves of electronic musician NSDOS based on the diabolical ternary rhythms of the Indian Ocean. In this confrontation between AI and the world of trance and spirits, the focus is on the delicate balance between traditional musical treasures and technological innovation.
 
**Psappho** is the third instance of the project Xenakis Reloaded, after Evryiali (2022) and AI-Komboï (2023), paying homage to Xenakis through the choice of a well known piece and an improvised extension using AI. As the continuation of a broader research and performance project developed by Lorenzo Colombo and Marco Fiorini, two artists and researchers dedicated to exploring the creative potentials at the intersection of human and artificial intelligence in music, this new chapter will use Psappha (1975) as its central material. The project will investigate new modes of cyber-human co-creativity by integrating Somax2 and gestural control into an immersive spatialized performance environment. Psappha will serve as the compositional and performative ground, with a focus on gestural and temporal density, rhythmic articulation, and structural complexity, core features of Xenakis’s language, and fertile terrain for this novel  interaction.
 
**SoVo II** is both a collaborative musical work and a new cocreative improvisation system conceived to expand the communicative terrain of human-machine collaboration, discussed for quite some time by George Lewis and Gerard Assayag during their encounters at preceding Improtechs. Designed and implemented in 2024-25 by Marco Fiorini, Damon Holzborn, George Lewis and Gerard Assayag, the SoVo system represents the convergence of decades of innovation in artificial intelligence and improvisation.  SoVo is not a fixed composition, but a dynamic environment where creative agency is distributed among human performers and improvising software agents operating in an integrated architecture combining Lewis’s Voyager system with the program Somax2 from Assayag’s reach.ircam.fr project. The system merges symbolic and audio-based interaction, combining in real time rule based algorithmic generation with machine learning and cognitive modeling, and uses machine listening and adaptive strategies both between the program and the musicians and between the So and Vo components. The system engages in the improvisational process with creative autonomy, where intention, memory, and form emerge through co-authored musical discourse.
 
**Solo for Sliding Trombone** This Performance presents an artistic research project exploring the performance of John Cage’s “Solo for Sliding Trombone” using AI generative tools within the Somax2 environment. The performance investigates the interplay between human interpretation, AI-assisted performance, and Cage’s core concepts of silence, indeterminacy, and unintentionality. By integrating AI agents as virtual performers and employing techniques like “coloring the silence” and “expansions,” the research aims to push the boundaries of Cage’s indeterminacy. This artistic research resulted in a unique set of improvised performances, captured and presented in a box set with 7 distinct tracks, showcasing the dynamic interplay between human and AI creativity within the framework of Cage’s innovative musical philosophy.
 
**Deleph** se promène dans l'univers entre le son et le langage, et plutôt dans ces régions où le langage, dès l’origine, touche à l’absurde. Volontairement, nous avons choisi un discours public très récent qui, quoique complètement inepte, a été pris au sérieux par des milliers de gens.
C’est pourquoi il est tentant de s’inspirer de ce qu’auraient pu en faire Offenbach, le Schwitters de la Ursonate, le Robert Erickson de General Speech, ou Jaap Blonk.
 
**NaN - Not a Number** is an immersive sonic experience, a real-time dialogue between the human voice and artificial intelligence, where the concept of feedback becomes a dynamic and creative principle. This performance-concert explores co-creation between the organic and the synthetic—an unpredictable fusion of the living presence of the voice and the responsiveness of an AI agent system that transforms, reacts to, and reshapes sound into a continuum of sonic metamorphosis. The performer, as the primary acoustic source, interacts with a latent AI environment that analyzes, processes, and reinterprets her vocality, generating an ever-evolving sound ecosystem. Every breath, every timbral inflection, and every harmonic extension propagates through space, giving rise to mutable and dynamic sonic configurations. The performance unfolds in an expanded and immersive environment, enriched by a resonant proscenium made of diverse materials (metal, glass, paper), which serve as surfaces for sound diffusion and vibration. These elements become active parts of the concert, amplifying and transforming the sound into a material choreography that engages the audience in a multi-sensory experience. The result is a seamless sonic flow, where the boundary between human and machine dissolves, giving way to a new, ever-transforming sonic organism. NaN is not just a musical performance—it is an experiment in symbiosis between voice and artificial intelligence, a journey into the unexpected, the indeterminate, and the beauty of emergent interaction.
 
**Taideji** In this piece, the collaboration between the performers is rooted in an exploration of the dynamic relationship between acoustic instruments and electronics, incorporating the interactive possibilities offered by Somax2. The sonic palette of the piano at times low, distorted, and percussive, at others bright and resonant, interacts with the rich colors of the percussion, shaping a musical journey marked by strong contrasts and an ever-shifting energy, oscillating between density and rarefaction. On the electronic side, the two acoustic instruments are processed in real time through various techniques, while Somax2 establishes a sensitive and reactive connection between instrumental performance and generated sound material, enriching the dialogue between human and machine.
 
**In albireo luogo…** "Dans cette nouvelle œuvre, j’explore les relations entre écriture, improvisation et électronique, dans la continuité de ma recherche sur les combinaisons dynamiques entre jeu instrumental, traitement en temps réel, espace d’écoute et interaction avec des agents cocréatifs. La rencontre entre deux solistes d’exception jouant le même instrument puissant, la contrebasse, avec leur style et leur énergie propres, et le logiciel Somax2, ouvre un monde sonore fascinant et un champ d’interactions improvisées où émergent des textures spatiales et des formes inédites. Cette coévolution du parcours formel repose sur une « musicalité partagée », fondée sur l’écoute, la réactivité et une intelligence collective, humaine comme machinique. Le projet s’inscrit dans une démarche expérimentale, relevant d’un défi à la fois musical et esthétique. Il mobilise des processus croisés et des comportements coopératifs qui génèrent des actions et échanges imprévus. Le matériau sonore devient un point d’ancrage, un levier pour dessiner des trajectoires et transformations articulant différents plans sonores. Entre détail de la notation et liberté interprétative, l’équilibre recherché est à la fois instable, subtil et incisif. La dimension électronique intensifie cette tension et ambiguïté, en instaurant un système d’interaction cyclique, organique et imprévisible. Dans l’espace sonore tridimensionnel, rendu possible par une spatialisation ambisonique, l’intégration d’agents spatialisés (Somax2) génère une interconnexion dynamique entre les parties écrites et improvisées, les inscrivant dans une logique d’interaction spatiale en constante évolution.” Lara Morciano, Juin 2025
 
Un poème de Chat-GPT:  
(Albireo est une étoile double dans la constellation du cygne)  
In albireo luogo  
Dans un lieu albireo, rien n’est fixe.  
Deux lumières cohabitent, l’une froide, l’autre brûlante —  
elles tournent l’une autour de l’autre, sans jamais se fondre.  
C’est un point d’équilibre instable,  
une tension suspendue entre contraste et fusion.  
Ici, les frontières entre le composé et l’improvisé se dissolvent.  
Le geste écrit rejoint le souffle libre,  
le traitement électronique devient partenaire de jeu.  
C’est un terrain mouvant,  
où la forme n’est jamais donnée mais toujours en devenir.  
Le lieu albireo est un espace d’écoute active,  
de co-présence sensible entre humains et machines,  
où chaque son porte la trace d’une décision partagée,  
et chaque silence, celle d’un dialogue en suspens.  
C’est là, dans ce lieu imaginaire mais audible,  
que cette œuvre prend naissance :  
entre précision et dérive,  
entre structure et surprise,  
entre étoile bleue et étoile dorée.  
 
**Reachin’Marseille** La série de performances REACHing OUT dont fait partie REACHin’Marseille célèbre, tout autour du monde, l’improvisation la plus jubilatoire autour de grandes personnalités musicales invitées accompagnée des Who/Men, des musiciens-chercheurs avec leurs machines dopées aux algorithmes d’IA créatives. Le programme de recherche et création REACH à l’initiative de cette nouvelle forme de performance, formule l’hypothèse de la co créativité entre agents de ces interactions improvisées incorporant des machines, comme une
sorte de réalité mixte : construisant une forme musicale toujours renouvelée, surgissant d'un matériau sonore co-construit à la fois imprévisible et contrôlé, du bruissement d’aile à l’explosion volcanique. Et si l’humain et la machine se rêvaient l’un l’autre, hybridant l’énergie créative humaine avec les processus d’écoute et d’apprentissage croisés et leurs boucles de rétroaction, en pur plaisir ?
Comme le dit Joëlle Léandre, régulièrement invitée de REACHing OUT, parlant de ces concerts : « Une vraie rencontre, une jubilation... C’est un risque et un moment unique, infini ! C’est sans doute chercher et peut-être trouver... Au fond, c’est "savoir ne pas savoir". »
Joëlle Léandre

### Installations-Performances
 
**WWW** La notion de WWW (« Wood Wide Web ») ou « Internet des arbres » repose sur la découverte que les arbres et les plantes ne sont pas des entités isolées, mais qu'ils entretiennent des relations complexes et symbiotiques entre eux et avec d'autres organismes. Cette expression est utilisée pour décrire le réseau complexe de communication et d'échange de nutriments qui existe entre les arbres, les plantes, les champignons et d'autres organismes vivants dans les écosystèmes forestiers. Cette communication souterraine joue un rôle essentiel dans le partage de ressources, la résistance aux maladies et la survie des plantes dans la nature. Ainsi, une autorégulation s'opère entre les différents composants de la forêt, façonnant un univers riche en interactions et en diversité. Cette idée constitue le fondement et la source d'inspiration de cette composition/improvisation avec le Spat’Sonore, que l’on peut définir comme un vaste instrument tentaculaire avec des ramifications comme « des plantes grimpantes en tubes de cuivre coiffées de pavillons-corolles, forment un igloo sonore dans lequel vient s’installer l’auditeur. Des machinistes se postent aux cerveaux à pistons de leur spat’ » . Les instruments spatialisés de l'ensemble Spat’Sonore, conjugués à l'électronique immersive en temps réel créeront un réseau de communication complexes entre tous les éléments du système. L’autorégulation sera assuré par des agents spatiaux de type Somax2Collider, dans un système de haut-parleurs distribués, jouant tout au long de la performance en tenant compte non seulement des descripteurs audio, mais aussi de la localisation des sons émis par les différents instruments de l’ensemble. 

**La Batterie fragile**

Conçue en porcelaine biscuit par le plasticien, écrivain et metteur en scène [Yves Chaudouët](http://www.dda-aquitaine.org/fr/yves-chaudouet/), la Batterie fragile est une sculpture qui demande des égards particuliers. Sorte d’ « oxymore musical », comme l’évoque le philosophe Pierre Sauvanet, elle se voulait initialement simple invitation à la rêverie. C’était sans compter sans le désir des musicien·nes de s’en emparer. Elle est donc régulièrement activée par des percussionnistes. Bernard Lubat, Valentina Magaletti, Sylvain Darrifourcq, Julian Sartorius, Aurélien Gignoux, Iker Idoate, Amélie Grould… jouent avec les limites de sa fragilité dans des formes solo présentées sur les scènes de musiques actuelles.
Le prototype de la Batterie fragile, à l’ÉSAD-Pyrénées en 2016, fait partie de la collection du FRAC Nouvelle-Aquitaine MÉCA. La V2 de Batterie fragile, développée avec les céramistes de l’ENSAD-Limoges, est celle qui sera activée pendant les journées d’Improtech.
La batterie fragile sera jouée par Thierry Miroglio, Aurélien Gignoux, avec divers musiciens dont Jean-Brice Godet (Clarinette), Nosfell (voix, texte) et le système d'IA générative cocréatif Somax2.





### Workshops

**[Alain Blesing, Claudie Boucau](https://alainblesing2.wixsite.com/mysite/about-3)** 

Alain Blesing, Guitare électrique, Somax2  
Claudie Boucau, flutes


**[Mari Kimura](https://www.marikimura.com/)** (UC Irvine) **MUGIC®: endless possibilities extending musical expression**

MUGIC® is a 9-axis motion sensor similar to other generic 9-axis sensors available on the market. However, what sets MUGIC® apart is its comprehensive, user-friendly design. Created by violinist and composer Mari Kimura, MUGIC® is a turnkey product that allows musicians to create their art immediately without requiring extensive programming or electrical engineering skills. The first version of MUGIC® sold out following a significant bulk order from the Lincoln Center in NYC this spring. As MUGIC® v.2 is under development, Kimura will demonstrate the importance of fostering a community around new technology and how MUGIC® users are expanding its application not only in music but also in other forms of art and beyond.
Workshops / Performances Studio 5 du 3 au 5 dec

**[Mikhail Malt](https://www.ircam.fr/person/mikhail-malt)** (Ircam), **PURE MALT: Augmented Improvisations, from instrumental gesture to telematics** 

This workshop highlights two research-creation projects that explore new frontiers in musical improvisation through interactive technologies and the internet. These case studies analyze how digital environments can not only expand the expressive possibilities of instrumentalists, but also create a shared performance space that transcends geographical limitations.  
The first research project focuses on improvisation with traditional instruments and artificial intelligence. Li-Chin Li plays the sheng (Chinese mouth organ) in interaction with the Somax2 interactive system. This system, capable of learning and responding in real time, becomes a full-fledged improvisational partner. We will show how the combination of the ancestral gesture of the sheng with the generative responses of the machine opens up a field of hybrid sound exploration, combining tradition and innovation.  
The second research project explores improvisation in a telematic performance. Mikhaïl Malt, in Paris in Studio 5 with the Somax2 environment and generative electronics, interacts live with flutist Cássia Carrascoza Bomfim, located in São Paulo. This configuration serves as a laboratory for examining the creative challenges and opportunities related to network latency, synchronization, and establishing a shared listening experience despite distance. The workshop will highlight strategies developed to transform technical constraints, such as delays and audio artifacts, into compositional and expressive elements.  
This workshop explores the future of improvisational practice by comparing two experiences: one focused on extending instrumental gesture and the other on delocalizing interaction. It examines how modern technologies are transforming relationships between performers, altering human-machine interactions, and redefining spaces for collective musical creation.


### Lectures


**[Pierre Couprie](https://www.pierrecouprie.fr/)** (Evry Paris-Saclay University) **Gestures in Electronic Improvised Music**

This presentation revisits Philippe Descola’s concept of “worlding,” emphasizing the creative process through which improvising musicians construct their own sonic environments. In electronic improvisation, the musician actively develops and refines their instrument, adapting its interfaces and performance techniques, thereby continuously shaping their unique musical world.
The discussion first examines the role of the instrument in electronic improvisation, highlighting its hybrid nature, which integrates electro-mechanical, analog, and digital elements. Unlike traditional instruments, electronic instruments evolve through rehearsals and performances, reflecting Thor Magnuson’s perspective on the genetic development of digital instruments and Georgina Born’s phenomenological view of performance as an ongoing process. The instrument is not a static entity but a “configuration” in Foucault’s sense—defined not by its structure alone but by its temporal evolution. Furthermore, electronic music devices function as complex networks of hardware, software, and protocols, challenging conventional definitions of musical instruments and redefining how musicians interact with their tools.
The second key concept, the trace, explores how performance documentation transforms into an artifact. In performance analysis, musicologists work with traces—recordings that capture an event and are subsequently shaped through selection, editing, and mastering. Alessandro Arbo’s distinction between a “recorded-document” and a “recorded-work” is particularly relevant in this context, as recordings of improvisation are not mere reproductions but constructed objects that reflect interpretive choices.
The final section addresses the complexities of musical gestures in electronic improvisation. Gestures are multifaceted, spanning micro-movements to full-body actions, and serving various functions, from sound production to expressive or auxiliary roles. Some gestures are directly linked to instruments, while others exist internally as mental representations. Moreover, gestures operate across multiple modalities, such as the visual gestures seen in Iannis Xenakis’ scores, which simultaneously function as musical gestures. The definition of gesture varies across disciplines: for musicians, gestures are tied to their instrument and performance setup; for scientists, they are studied in terms of their functional outcomes; for computer scientists, they are examined in relation to sensor-mapping and interaction; and for sociologists and musicologists, they are analyzed within broader performance contexts.
Given the inherent challenges in defining and analyzing gestures, this presentation introduces the concept of a “gestural chain” or “catena,” drawing from geological metaphors to illustrate how gestures influence and build upon one another in a non-hierarchical structure. This interconnected network of gestures shapes the act of performance, encompassing movements made by the musician, interactions with the instrument, and the perception of these actions by the audience. The presentation is accompanied by audiovisual examples, including an improvisation, as well as a performance where gestures of diffusion are explored within a spatialized orchestration setup. Ultimately, the study of music, particularly in improvisation, is not merely an analysis of sound objects but of the complex web of gestures that bring music into being.



**[Nicolas Brochec](https://nicolasbrochec.com)**, Tokyo University of the Arts Geidai, **[Marco Fiorini](https://forum.ircam.fr/profile/fiorini/)** (Ircam) **Real-Time Recognition of Instrument Playing Techniques for Mixed Music and CoCreative Interaction**

We are going to detail the techniques, methodologies, and outcomes that led to the development of an interactive system based on real-time Instrumental Playing Technique (IPT) recognition. Starting from exploratory studies on the flute, we will discuss soundbank recording, data format, and data augmentation, as well as state-of-the-art machine learning model architectures developed in our research. By connecting our model to the co-creative AI system Somax2, we are able to interact with generative agents by means of real-time recognition of IPT classes, adding a new dimension to its interaction paradigm and addressing potential scenarios of co-creative human-machine interaction in mixed music for improvisation and composition.



**[Oded Ben Tal](https://www.kingston.ac.uk/about/staff/dr-oded-ben-tal)** (Kingston University), **[David Dolan](https://www.gsmd.ac.uk/staff/professor-david-dolan)** (Guildhall School of Music), **The Odd Couple: Human and AI Making Music in the Moment**
This performance/talk will present an ongoing collaboration between composer Oded Ben-Tal and pianist David Dolan. Ben-Tal has been developing an AI-inspired system (JHAIMI – Joint Human AI Music Improvisation) that ‘listens’ to the pianist (extracting musical data from microphone input) and generates responses in real-time during the performance. The responses combine generative compositional processes on the one hand and real-time musical inferences about the pianist’s improvisation on the other. The aim is to create a strong, sophisticated and nuanced musical dialogue between human and machine. Dolan’s improvisations are based on an expanded tonal-modal idiom but do not conform to a specific musical style nor adhere to a preplanned scheme such as a chord progression, agreed tempo, key, or meter. The result is a new form of musical dialogue, created by the possibilities of new technology and drawing on the wealth of 300 years of music making. Ben-Tal is adjusting parameters in the system during the performance to shape larger-scale aspects, but the moment-to-moment generation of musical material is done automatically by JHAIMI. [See also](https://www.aesthetics.mpg.de/en/newsroom/events/events/article/creative-musical-dialogues-between-human-and-machine-en.html).



<br>

---


<br>
<br>
<p align="center">
  <img src="../images/Logo_ikPT_MSS.png" width="400">
</p>

<br>
<br>

